{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Fetching Different Data from Swiss Parliament Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data from Webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 01. Fetch Affair IDs by Legislative Period\n",
    "###########################################\n",
    "\n",
    "# legislative periods of interest \n",
    "legislative_periods = [49,50,51,52]\n",
    "\n",
    "# web service URL and headers\n",
    "base_url = 'https://ws-old.parlament.ch/votes/affairs'\n",
    "headers = {'User-Agent': 'Mozilla/5.0', # required to mimic browser request\n",
    "           'language': 'de'}\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# repeat GET requests for each legislative period\n",
    "for period in legislative_periods:\n",
    "    params = {\n",
    "        'legislativePeriodFilter': period,\n",
    "        'pageNumber': 1,\n",
    "        'format': 'json'\n",
    "        }\n",
    "\n",
    "    # loop through affairs pages\n",
    "    all_pages = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        # extract JSON and convert to df\n",
    "        data = response.json()\n",
    "        df_page = pd.DataFrame(data)\n",
    "\n",
    "        all_pages.append(df_page)\n",
    "\n",
    "        # loop progression\n",
    "        print(f\"Period {period} - Collecting data for page {params['pageNumber']}\")\n",
    "        \n",
    "        # go to next page if it exists\n",
    "        if df_page['hasMorePages'].iloc[-1] == True:\n",
    "            params['pageNumber'] += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # combine all pages\n",
    "    df = pd.concat(all_pages, ignore_index=True)\n",
    "    df['legislature'] = period\n",
    "    df.drop(columns='hasMorePages', inplace=True)\n",
    "\n",
    "    # append\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# concatenate\n",
    "all_ids_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# 02. Fetch Votes for Each Affair\n",
    "#################################\n",
    "\n",
    "# set max workers\n",
    "max_workers = 40 # frequently used for I/O-bound tasks\n",
    "\n",
    "# select affair ids for which votes should be fetched\n",
    "affair_ids = pd.read_parquet('all_affair_ids.parquet')['id']\n",
    "\n",
    "# Function to fetch votes for a single affair\n",
    "def fetch_votes(affair_id):\n",
    "    params = {'format': 'json', 'pageNumber': 1}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0', 'language': 'de'}\n",
    "    base_url = f'https://ws-old.parlament.ch/votes/affairs/{affair_id}'\n",
    "    pages = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # fetch JSON & convert it to df\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            df_page = pd.DataFrame(data)\n",
    "            pages.append(df_page)\n",
    "\n",
    "            # check whether there are more pages\n",
    "            if df_page['affairVotes'].iloc[-1].get('hasMorePages', False):\n",
    "                params['pageNumber'] += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # combine data from all pages\n",
    "        df = pd.concat(pages, ignore_index=True)\n",
    "\n",
    "        # extract fields to differentiate between subvotes for same affair (e.g. first vote vs final vote)\n",
    "        df['date'] = df['affairVotes'].apply(lambda x: x.get('date'))\n",
    "        df['divisionText'] = df['affairVotes'].apply(lambda x: x.get('divisionText'))\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch votes for affair_id {affair_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize HTTPS requests\n",
    "all_dfs = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # submit all jobs and map futures to affair_ids\n",
    "    futures = {executor.submit(fetch_votes, aid): aid for aid in affair_ids}\n",
    "\n",
    "    # Collect completed results with progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            all_dfs.append(result)\n",
    "\n",
    "# concatenate\n",
    "all_votes_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 03. Get Information on Each Affair\n",
    "#####################################\n",
    "\n",
    "# set max workers\n",
    "max_workers = 3 # if the number is too high, the request get blocked\n",
    "\n",
    "# select affair ids for which votes should be fetched\n",
    "affair_ids = pd.read_parquet('all_affair_ids.parquet')['id']\n",
    "\n",
    "# Function to fetch JSON summary of single affair\n",
    "def fetch_affair_summary(affair_id): \n",
    "    params = {'format': 'json', 'pageNumber': 1}\n",
    "    base_url = f'https://ws-old.parlament.ch/affairs/{affair_id}'\n",
    "\n",
    "    try: \n",
    "        # fetch JSON\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch summary for affair_id {affair_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize HTTPS requests\n",
    "all_affair_summaries = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # submit all jobs and map futures to affair_ids\n",
    "    futures = {executor.submit(fetch_affair_summary, aid): aid for aid in affair_ids}\n",
    "\n",
    "    # Collect completed results with progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            all_affair_summaries.append(result)\n",
    "\n",
    "# Combine summary data for all affairs in single df and save it\n",
    "all_affair_summaries_df = pd.DataFrame(all_affair_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# 04. Get All Councillors by Legislative Periods\n",
    "#################################################\n",
    "\n",
    "# legislative periods of interest \n",
    "legislative_periods = [47,48,49,50,51,52]\n",
    "\n",
    "# web service URL and headers\n",
    "base_url = 'https://ws-old.parlament.ch/councillors/historic'\n",
    "headers = {'User-Agent': 'Mozilla/5.0', # required to mimic browser request\n",
    "           'language': 'de'}\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# repeat GET requests for each legislative period\n",
    "for period in legislative_periods:\n",
    "    params = {\n",
    "        'legislativePeriodFromFilter': period,\n",
    "        'pageNumber': 1,\n",
    "        'format': 'json'\n",
    "        }\n",
    "\n",
    "    # loop through councillor pages\n",
    "    all_pages = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        # extract JSON and convert to df\n",
    "        data = response.json()\n",
    "        df_page = pd.DataFrame(data)\n",
    "\n",
    "        all_pages.append(df_page)\n",
    "\n",
    "        # loop progression\n",
    "        print(f'Period {period} - Collecting data for page {params['pageNumber']}')\n",
    "        \n",
    "        # go to next page if it exists\n",
    "        if df_page['hasMorePages'].iloc[-1] == True:\n",
    "            params['pageNumber'] += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # combine all pages\n",
    "    df = pd.concat(all_pages, ignore_index=True)\n",
    "    df['legislatureFrom'] = period\n",
    "    df.drop(columns='hasMorePages', inplace=True)\n",
    "\n",
    "    # append\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# concatenate\n",
    "all_ids_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# 06. Fetch commission information\n",
    "#################################\n",
    "\n",
    "# legislative periods of interest \n",
    "legislative_periods = [52]\n",
    "\n",
    "# web service URL and headers\n",
    "base_url = 'https://ws-old.parlament.ch/committees'\n",
    "headers = {'User-Agent': 'Mozilla/5.0', # required to mimic browser request\n",
    "           'language': 'de'}\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# GET request for committee ids\n",
    "params = {\n",
    "        'currentOnly': 'true',\n",
    "        'pageNumber': 1,\n",
    "        'format': 'json'\n",
    "        }\n",
    "\n",
    "# loop through councillor pages\n",
    "all_pages = []\n",
    "\n",
    "while True:\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    \n",
    "    # extract JSON and convert to df\n",
    "    data = response.json()\n",
    "    df_page = pd.DataFrame(data)\n",
    "\n",
    "    all_pages.append(df_page)\n",
    "    \n",
    "    # go to next page if it exists\n",
    "    if df_page['hasMorePages'].iloc[-1] == True:\n",
    "        params['pageNumber'] += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# combine all pages\n",
    "df = pd.concat(all_pages, ignore_index=True)\n",
    "\n",
    "# filter to only keep national council committees\n",
    "df['council'] = df['council'].apply(lambda x: x['abbreviation'])\n",
    "df = df[df['council'] == 'NR']\n",
    "\n",
    "# extract ids of relevant committees\n",
    "committee_ids = df['id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
